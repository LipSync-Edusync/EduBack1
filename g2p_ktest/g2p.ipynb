{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66764db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e660726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d345883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLASLGlossPoseDataset(Dataset):\n",
    "    def __init__(self, json_path, video_dir, gloss_vocab=None, max_samples=100, cache_dir=\"pose_cache\", force_reprocess=False):\n",
    "        \"\"\"\n",
    "        WLASL Gloss to Pose Dataset\n",
    "        Args:\n",
    "            json_path: Path to WLASL_v0.3.json\n",
    "            video_dir: Directory containing video files\n",
    "            gloss_vocab: Existing gloss vocabulary (or create new)\n",
    "            max_samples: Maximum samples to process\n",
    "            cache_dir: Directory to store extracted poses\n",
    "            force_reprocess: Reprocess even if cached exists\n",
    "        \"\"\"\n",
    "        self.video_dir = video_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.force_reprocess = force_reprocess\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if gloss_vocab is None:\n",
    "            self.gloss_vocab = {}\n",
    "            for i, entry in enumerate(data):\n",
    "                self.gloss_vocab[entry['gloss']] = i\n",
    "        else:\n",
    "            self.gloss_vocab = gloss_vocab\n",
    "            \n",
    "        self.samples = []\n",
    "        for entry in data[:max_samples]:\n",
    "            gloss = entry['gloss']\n",
    "            for instance in entry['instances']:\n",
    "                video_id = instance['video_id']\n",
    "                video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n",
    "                if os.path.exists(video_path):\n",
    "                    self.samples.append((gloss, video_path, video_id))\n",
    "        \n",
    "        # Initialize MediaPipe Pose\n",
    "        self.mp_pose = mp.solutions.pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gloss, video_path, video_id = self.samples[idx]\n",
    "        cache_path = os.path.join(self.cache_dir, f\"{video_id}.pt\")\n",
    "        \n",
    "        # Load from cache or process video\n",
    "        if os.path.exists(cache_path) and not self.force_reprocess:\n",
    "            pose_seq = torch.load(cache_path)\n",
    "        else:\n",
    "            pose_seq = self.process_video(video_path)\n",
    "            torch.save(pose_seq, cache_path)\n",
    "        \n",
    "        return {\n",
    "            'gloss': self.gloss_vocab[gloss],\n",
    "            'pose': pose_seq,\n",
    "            'video_id': video_id\n",
    "        }\n",
    "    \n",
    "    def process_video(self, video_path):\n",
    "        # extract pose sequence from video using\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        pose_sequence = []\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Process frame with MediaPipe\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = self.mp_pose.process(frame_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Extract 33 pose landmarks (x, y, visibility)\n",
    "                landmarks = []\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    landmarks.extend([landmark.x, landmark.y, landmark.visibility])\n",
    "                pose_sequence.append(landmarks)\n",
    "            else:\n",
    "                # Pad with zeros if no detection\n",
    "                pose_sequence.append([0.0]*99)\n",
    "        \n",
    "        cap.release()\n",
    "        return torch.tensor(pose_sequence, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3697f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gloss2Pose(nn.Module):\n",
    "    def __init__(self, gloss_vocab_size, pose_dim=99):  # 33 keypoints * 3 (x, y, conf)\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(gloss_vocab_size, 128)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Conv1d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, pose_dim, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, gloss_seq):\n",
    "        # Input shape: (batch_size, seq_len)\n",
    "        x = self.embed(gloss_seq)  # (batch_size, seq_len, embed_dim=128)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, embed_dim, seq_len)\n",
    "        x = self.conv(x)  # (batch_size, pose_dim, seq_len)\n",
    "        return x.permute(0, 2, 1)  # (batch_size, seq_len, pose_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ac244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "SAVE_PATH = \"g2p_model_trained.pth\"\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to pad pose sequences\"\"\"\n",
    "    glosses = torch.tensor([item['gloss'] for item in batch])\n",
    "    poses = [item['pose'] for item in batch]\n",
    "    video_ids = [item['video_id'] for item in batch]\n",
    "    \n",
    "    poses_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        poses, batch_first=True, padding_value=0.0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gloss': glosses,\n",
    "        'pose': poses_padded,\n",
    "        'pose_lengths': torch.tensor([len(p) for p in poses]),\n",
    "        'video_id': video_ids\n",
    "    }\n",
    "\n",
    "def train():\n",
    "    # Initialize dataset and loader\n",
    "    dataset = WLASLGlossPoseDataset(\n",
    "        json_path=\"archive/WLASL_v0.3.json\",\n",
    "        video_dir=\"archive/videos\",\n",
    "        max_samples=1000\n",
    "    )\n",
    "    \n",
    "    # Create train/test split\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Gloss2Pose(\n",
    "        gloss_vocab_size=len(dataset.gloss_vocab),\n",
    "        pose_dim=99\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "            gloss = batch['gloss'].to(DEVICE)  # shape: (batch_size,)\n",
    "            pose = batch['pose'].to(DEVICE)    # shape: (batch_size, max_seq_len, 99)\n",
    "            lengths = batch['pose_lengths']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - need to unsqueeze gloss if it's 1D\n",
    "            if gloss.dim() == 1:\n",
    "                gloss = gloss.unsqueeze(1)  # shape: (batch_size, 1)\n",
    "            \n",
    "            outputs = model(gloss)  # shape: (batch_size, seq_len, 99)\n",
    "            \n",
    "            # Calculate loss only on valid frames\n",
    "            loss = 0.0\n",
    "            for i in range(outputs.size(0)):\n",
    "                # Ensure we don't exceed sequence length\n",
    "                valid_len = min(lengths[i], outputs.size(1))\n",
    "                loss += criterion(outputs[i, :valid_len], pose[i, :valid_len])\n",
    "            \n",
    "            loss /= outputs.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=f\"Val Epoch {epoch+1}\"):\n",
    "                gloss = batch['gloss'].to(DEVICE)\n",
    "                pose = batch['pose'].to(DEVICE)\n",
    "                lengths = batch['pose_lengths']\n",
    "                \n",
    "                if gloss.dim() == 1:\n",
    "                    gloss = gloss.unsqueeze(1)\n",
    "                \n",
    "                outputs = model(gloss)\n",
    "                \n",
    "                batch_loss = 0.0\n",
    "                for i in range(outputs.size(0)):\n",
    "                    valid_len = min(lengths[i], outputs.size(1))\n",
    "                    batch_loss += criterion(outputs[i, :valid_len], pose[i, :valid_len])\n",
    "                \n",
    "                val_loss += batch_loss.item() / outputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(test_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_g2p_model.pth\")\n",
    "            print(\"Saved new best model\")\n",
    "    \n",
    "    print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f302a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749793641.327379  141639 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1749793641.378231  141943 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.230.02), renderer: Quadro P2000/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1749793641.456598  141927 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749793641.572985  141938 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Train Epoch 1:   0%|          | 0/181 [00:00<?, ?it/s]W0000 00:00:1749793642.430326  141942 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Train Epoch 1:  56%|█████▋    | 102/181 [3:25:31<2:37:18, 119.48s/it][NULL @ 0x5b138e652c00] Invalid NAL unit size (71678 > 10776).\n",
      "[NULL @ 0x5b138e652c00] missing picture in access unit with size 10780\n",
      "[h264 @ 0x5b138dff4480] Invalid NAL unit size (71678 > 10776).\n",
      "[h264 @ 0x5b138dff4480] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b138d9b9d40] stream 1, offset 0x2a27a7: partial file\n",
      "Train Epoch 1:  57%|█████▋    | 104/181 [3:29:28<2:32:39, 118.96s/it][h264 @ 0x5b138e546a40] Invalid NAL unit size (745 > 472).\n",
      "[h264 @ 0x5b138e546a40] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b137ccd9400] stream 1, offset 0x3b468: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b137ccd9400] stream 1, offset 0x3b7d3: partial file\n",
      "Train Epoch 1: 100%|██████████| 181/181 [6:02:58<00:00, 120.32s/it]  \n",
      "Val Epoch 1: 100%|██████████| 46/46 [1:30:48<00:00, 118.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.2468, Val Loss = 0.0390\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 181/181 [00:03<00:00, 55.20it/s]\n",
      "Val Epoch 2: 100%|██████████| 46/46 [00:00<00:00, 76.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0274, Val Loss = 0.0239\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 181/181 [00:03<00:00, 56.80it/s]\n",
      "Val Epoch 3: 100%|██████████| 46/46 [00:00<00:00, 75.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0228, Val Loss = 0.0225\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 181/181 [00:03<00:00, 56.47it/s]\n",
      "Val Epoch 4: 100%|██████████| 46/46 [00:00<00:00, 74.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.0218, Val Loss = 0.0220\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|██████████| 181/181 [00:03<00:00, 55.78it/s]\n",
      "Val Epoch 5: 100%|██████████| 46/46 [00:00<00:00, 75.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.0212, Val Loss = 0.0217\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|██████████| 181/181 [00:03<00:00, 57.05it/s]\n",
      "Val Epoch 6: 100%|██████████| 46/46 [00:00<00:00, 76.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.0208, Val Loss = 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|██████████| 181/181 [00:03<00:00, 56.39it/s]\n",
      "Val Epoch 7: 100%|██████████| 46/46 [00:00<00:00, 74.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.0205, Val Loss = 0.0216\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|██████████| 181/181 [00:03<00:00, 56.89it/s]\n",
      "Val Epoch 8: 100%|██████████| 46/46 [00:00<00:00, 76.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.0201, Val Loss = 0.0214\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|██████████| 181/181 [00:03<00:00, 56.70it/s]\n",
      "Val Epoch 9: 100%|██████████| 46/46 [00:00<00:00, 74.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.0198, Val Loss = 0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10: 100%|██████████| 181/181 [00:03<00:00, 56.83it/s]\n",
      "Val Epoch 10: 100%|██████████| 46/46 [00:00<00:00, 78.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.0197, Val Loss = 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11: 100%|██████████| 181/181 [00:03<00:00, 56.18it/s]\n",
      "Val Epoch 11: 100%|██████████| 46/46 [00:00<00:00, 77.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.0195, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12: 100%|██████████| 181/181 [00:03<00:00, 57.53it/s]\n",
      "Val Epoch 12: 100%|██████████| 46/46 [00:00<00:00, 61.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.0193, Val Loss = 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13: 100%|██████████| 181/181 [00:03<00:00, 57.63it/s]\n",
      "Val Epoch 13: 100%|██████████| 46/46 [00:00<00:00, 76.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.0185, Val Loss = 0.0213\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14: 100%|██████████| 181/181 [00:03<00:00, 57.24it/s]\n",
      "Val Epoch 14: 100%|██████████| 46/46 [00:00<00:00, 73.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.0183, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 15: 100%|██████████| 181/181 [00:03<00:00, 58.39it/s]\n",
      "Val Epoch 15: 100%|██████████| 46/46 [00:00<00:00, 71.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 0.0183, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 16: 100%|██████████| 181/181 [00:03<00:00, 57.45it/s]\n",
      "Val Epoch 16: 100%|██████████| 46/46 [00:00<00:00, 75.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 0.0182, Val Loss = 0.0213\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 17: 100%|██████████| 181/181 [00:03<00:00, 56.77it/s]\n",
      "Val Epoch 17: 100%|██████████| 46/46 [00:00<00:00, 78.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 0.0182, Val Loss = 0.0213\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 18: 100%|██████████| 181/181 [00:03<00:00, 57.87it/s]\n",
      "Val Epoch 18: 100%|██████████| 46/46 [00:00<00:00, 81.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 0.0182, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 19: 100%|██████████| 181/181 [00:03<00:00, 57.10it/s]\n",
      "Val Epoch 19: 100%|██████████| 46/46 [00:00<00:00, 76.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 0.0182, Val Loss = 0.0213\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 20: 100%|██████████| 181/181 [00:03<00:00, 57.80it/s]\n",
      "Val Epoch 20: 100%|██████████| 46/46 [00:00<00:00, 78.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 0.0181, Val Loss = 0.0213\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 21: 100%|██████████| 181/181 [00:03<00:00, 56.90it/s]\n",
      "Val Epoch 21: 100%|██████████| 46/46 [00:00<00:00, 73.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss = 0.0180, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 22: 100%|██████████| 181/181 [00:03<00:00, 56.83it/s]\n",
      "Val Epoch 22: 100%|██████████| 46/46 [00:00<00:00, 78.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss = 0.0181, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 23: 100%|██████████| 181/181 [00:03<00:00, 57.42it/s]\n",
      "Val Epoch 23: 100%|██████████| 46/46 [00:00<00:00, 74.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss = 0.0181, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 24: 100%|██████████| 181/181 [00:03<00:00, 57.24it/s]\n",
      "Val Epoch 24: 100%|██████████| 46/46 [00:00<00:00, 76.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss = 0.0181, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 25: 100%|██████████| 181/181 [00:03<00:00, 56.92it/s]\n",
      "Val Epoch 25: 100%|██████████| 46/46 [00:00<00:00, 78.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss = 0.0180, Val Loss = 0.0213\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 26: 100%|██████████| 181/181 [00:03<00:00, 56.15it/s]\n",
      "Val Epoch 26: 100%|██████████| 46/46 [00:00<00:00, 75.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 27: 100%|██████████| 181/181 [00:03<00:00, 58.42it/s]\n",
      "Val Epoch 27: 100%|██████████| 46/46 [00:00<00:00, 78.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 28: 100%|██████████| 181/181 [00:03<00:00, 57.77it/s]\n",
      "Val Epoch 28: 100%|██████████| 46/46 [00:00<00:00, 73.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 29: 100%|██████████| 181/181 [00:03<00:00, 56.71it/s]\n",
      "Val Epoch 29: 100%|██████████| 46/46 [00:00<00:00, 75.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 30: 100%|██████████| 181/181 [00:03<00:00, 58.11it/s]\n",
      "Val Epoch 30: 100%|██████████| 46/46 [00:00<00:00, 75.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 31: 100%|██████████| 181/181 [00:03<00:00, 57.70it/s]\n",
      "Val Epoch 31: 100%|██████████| 46/46 [00:00<00:00, 78.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 32: 100%|██████████| 181/181 [00:03<00:00, 57.92it/s]\n",
      "Val Epoch 32: 100%|██████████| 46/46 [00:00<00:00, 75.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 33: 100%|██████████| 181/181 [00:03<00:00, 57.36it/s]\n",
      "Val Epoch 33: 100%|██████████| 46/46 [00:00<00:00, 75.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 34: 100%|██████████| 181/181 [00:03<00:00, 57.39it/s]\n",
      "Val Epoch 34: 100%|██████████| 46/46 [00:00<00:00, 75.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 35: 100%|██████████| 181/181 [00:03<00:00, 57.71it/s]\n",
      "Val Epoch 35: 100%|██████████| 46/46 [00:00<00:00, 76.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 36: 100%|██████████| 181/181 [00:03<00:00, 58.08it/s]\n",
      "Val Epoch 36: 100%|██████████| 46/46 [00:00<00:00, 76.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 37: 100%|██████████| 181/181 [00:03<00:00, 57.32it/s]\n",
      "Val Epoch 37: 100%|██████████| 46/46 [00:00<00:00, 78.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 38: 100%|██████████| 181/181 [00:03<00:00, 57.47it/s]\n",
      "Val Epoch 38: 100%|██████████| 46/46 [00:00<00:00, 77.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 39: 100%|██████████| 181/181 [00:03<00:00, 55.94it/s]\n",
      "Val Epoch 39: 100%|██████████| 46/46 [00:00<00:00, 76.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 40: 100%|██████████| 181/181 [00:03<00:00, 57.62it/s]\n",
      "Val Epoch 40: 100%|██████████| 46/46 [00:00<00:00, 79.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 41: 100%|██████████| 181/181 [00:03<00:00, 58.10it/s]\n",
      "Val Epoch 41: 100%|██████████| 46/46 [00:00<00:00, 75.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 42: 100%|██████████| 181/181 [00:03<00:00, 57.82it/s]\n",
      "Val Epoch 42: 100%|██████████| 46/46 [00:00<00:00, 78.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 43: 100%|██████████| 181/181 [00:03<00:00, 57.24it/s]\n",
      "Val Epoch 43: 100%|██████████| 46/46 [00:00<00:00, 79.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 44: 100%|██████████| 181/181 [00:03<00:00, 57.41it/s]\n",
      "Val Epoch 44: 100%|██████████| 46/46 [00:00<00:00, 79.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Train Loss = 0.0179, Val Loss = 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 45: 100%|██████████| 181/181 [00:03<00:00, 57.73it/s]\n",
      "Val Epoch 45: 100%|██████████| 46/46 [00:00<00:00, 76.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 46: 100%|██████████| 181/181 [00:03<00:00, 57.84it/s]\n",
      "Val Epoch 46: 100%|██████████| 46/46 [00:00<00:00, 77.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 47: 100%|██████████| 181/181 [00:03<00:00, 57.22it/s]\n",
      "Val Epoch 47: 100%|██████████| 46/46 [00:00<00:00, 75.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 48: 100%|██████████| 181/181 [00:03<00:00, 57.49it/s]\n",
      "Val Epoch 48: 100%|██████████| 46/46 [00:00<00:00, 76.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss = 0.0179, Val Loss = 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 49: 100%|██████████| 181/181 [00:03<00:00, 58.17it/s]\n",
      "Val Epoch 49: 100%|██████████| 46/46 [00:00<00:00, 75.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.0179, Val Loss = 0.0212\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 50: 100%|██████████| 181/181 [00:03<00:00, 57.51it/s]\n",
      "Val Epoch 50: 100%|██████████| 46/46 [00:00<00:00, 78.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss = 0.0179, Val Loss = 0.0213\n",
      "Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba46919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGlossDataset3(Dataset): # takes from .pth\n",
    "    def __init__(self, processed_path):\n",
    "        \n",
    "        data = torch.load(processed_path, map_location=torch.device(\"cpu\"))\n",
    "        self.text_vocab  = data[\"text_vocab\"]\n",
    "        self.gloss_vocab = data[\"gloss_vocab\"]\n",
    "        self.inv_gloss   = data[\"inv_gloss\"]\n",
    "\n",
    "        # Pre‐tokenized (N, max_seq_len)\n",
    "        self.text_matrix  = data[\"text_matrix\"]\n",
    "        self.gloss_matrix = data[\"gloss_matrix\"]\n",
    "        \n",
    "        self.pose_matrix = data['pose_matrix']\n",
    "\n",
    "        assert self.text_matrix.size(0) == self.gloss_matrix.size(0), \"Mismatch in example count\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.text_matrix.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_indices  = self.text_matrix[idx]\n",
    "        gloss_indices = self.gloss_matrix[idx]\n",
    "        pose_indices = self.pose_matrix[idx]\n",
    "        return text_indices, gloss_indices, pose_indices\n",
    "\n",
    "    def decode_gloss(self, indices):\n",
    "        return \" \".join(\n",
    "            [self.inv_gloss.get(int(idx), \"<unk>\") for idx in indices if idx not in {0, 1, 2}]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4d23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2GlossTransformer(nn.Module):\n",
    "    def __init__(self, text_vocab_size, gloss_vocab_size):\n",
    "        super().__init__()\n",
    "        self.text_embed = nn.Embedding(text_vocab_size, 256)\n",
    "        self.gloss_embed = nn.Embedding(gloss_vocab_size, 256)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=256, nhead=8, num_encoder_layers=3, num_decoder_layers=3\n",
    "        ).to(device)\n",
    "        self.fc = nn.Linear(256, gloss_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.text_embed(src).permute(1,0,2) # S, B, E\n",
    "        tgt = self.gloss_embed(tgt).permute(1,0,2)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        return self.fc(output).permute(1,0,2) # B, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad09615",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c514eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "WLASL_DATASET = 'wlasl_dataset.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad4e3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2g_losses = []\n",
    "\n",
    "def train2():\n",
    "    dataset = TextGlossDataset3(WLASL_DATASET)\n",
    "    \n",
    "    t2g_model = Text2GlossTransformer(\n",
    "        len(dataset.text_vocab),\n",
    "        len(dataset.gloss_vocab)\n",
    "    ).to(device)\n",
    "    \n",
    "    t2g_optim = torch.optim.Adam(t2g_model.parameters(), lr=1e-4)\n",
    "    loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        for batch in loader:\n",
    "            text, gloss, pose = batch\n",
    "            text, gloss = text.to(device), gloss.to(device)\n",
    "            \n",
    "            t2g_optim.zero_grad()\n",
    "            decoder_input = torch.cat([\n",
    "                torch.ones_like(gloss[:, :1]) * dataset.gloss_vocab[\"<sos>\"],\n",
    "                gloss[:, :-1]\n",
    "            ], dim=1)\n",
    "            \n",
    "            gloss_logits = t2g_model(text, decoder_input)\n",
    "            \n",
    "            # FIXED: Make tensors contiguous before view\n",
    "            t2g_loss = F.cross_entropy(\n",
    "                gloss_logits.contiguous().view(-1, gloss_logits.size(-1)),\n",
    "                gloss.contiguous().view(-1),\n",
    "                ignore_index=0  # Optional: ignore padding index\n",
    "            )\n",
    "            t2g_loss.backward()\n",
    "            t2g_optim.step()\n",
    "            \n",
    "            t2g_losses.append(t2g_loss.item())\n",
    "        print(f\"Epoch {epoch+1}: T2G Loss={t2g_loss.item():.4f}\")\n",
    "        \n",
    "    return t2g_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "730a96ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/EDUSYNC/edu_back/edu_back/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: T2G Loss=2.4554\n",
      "Epoch 2: T2G Loss=2.5578\n",
      "Epoch 3: T2G Loss=2.6521\n",
      "Epoch 4: T2G Loss=2.6818\n",
      "Epoch 5: T2G Loss=2.6016\n",
      "Epoch 6: T2G Loss=2.5468\n",
      "Epoch 7: T2G Loss=2.5108\n",
      "Epoch 8: T2G Loss=2.5678\n",
      "Epoch 9: T2G Loss=2.5760\n",
      "Epoch 10: T2G Loss=2.9889\n",
      "Epoch 11: T2G Loss=2.6295\n",
      "Epoch 12: T2G Loss=2.5566\n",
      "Epoch 13: T2G Loss=2.8760\n",
      "Epoch 14: T2G Loss=2.5514\n",
      "Epoch 15: T2G Loss=2.5751\n",
      "Epoch 16: T2G Loss=2.5277\n",
      "Epoch 17: T2G Loss=2.5481\n",
      "Epoch 18: T2G Loss=2.5730\n",
      "Epoch 19: T2G Loss=2.4859\n",
      "Epoch 20: T2G Loss=2.5169\n",
      "Epoch 21: T2G Loss=2.5904\n",
      "Epoch 22: T2G Loss=2.5398\n",
      "Epoch 23: T2G Loss=2.4936\n",
      "Epoch 24: T2G Loss=2.8740\n",
      "Epoch 25: T2G Loss=2.8764\n",
      "Epoch 26: T2G Loss=2.8979\n",
      "Epoch 27: T2G Loss=3.0429\n",
      "Epoch 28: T2G Loss=2.7748\n",
      "Epoch 29: T2G Loss=2.8568\n",
      "Epoch 30: T2G Loss=2.7141\n"
     ]
    }
   ],
   "source": [
    "t2g_model = train2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a04784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(t2g_model.state_dict(), \"t2g_model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30283fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
